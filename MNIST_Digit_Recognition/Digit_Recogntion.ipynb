{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the dataset\n",
    "(X1, Y1), (X2, Y2) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process the dataset\n",
    "m_train = X1.shape[0]\n",
    "m_test = X2.shape[0]\n",
    "X_train = (X1.reshape(X1.shape[0],-1).T)/255\n",
    "Y_train_temp = Y1.reshape(Y1.shape[0],)\n",
    "Y_train = np.zeros((Y_train_temp.size,10))\n",
    "Y_train[np.arange(Y_train_temp.size),Y_train_temp] = 1\n",
    "Y_train = Y_train.T\n",
    "X_test = (X2.reshape(X2.shape[0],-1).T)/255\n",
    "Y_test_temp = Y2.reshape(Y2.shape[0],)\n",
    "Y_test = np.zeros((Y_test_temp.size,10))\n",
    "Y_test[np.arange(Y_test_temp.size),Y_test_temp] = 1\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining activation functions\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def leakyrelu(z):\n",
    "    s = np.where(z>0 , z , z*0.01)\n",
    "    activation_cache = (z)\n",
    "    return s, activation_cache\n",
    "\n",
    "def softmax(z):\n",
    "    s = np.exp(z)/np.sum(np.exp(z), axis = 0, keepdims = True)\n",
    "    activation_cache = (z)\n",
    "    return s, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X,Y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- Input image\n",
    "    Y -- Label of input image\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = np.random.randn(100,X.shape[0])*0.01\n",
    "    b1 = np.zeros((100,1), dtype = float)\n",
    "    W2 = np.random.randn(50,W1.shape[0])*0.01\n",
    "    b2 = np.zeros((50,1), dtype = float)\n",
    "    W3 = np.random.randn(25,W2.shape[0])*0.01\n",
    "    b3 = np.zeros((25,1), dtype = float)\n",
    "    W4 = np.random.randn(10,W3.shape[0])*0.01\n",
    "    b4 = np.zeros((10,1), dtype = float)\n",
    "\n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1,\"W2\" : W2, \"b2\" : b2,\"W3\" : W3, \"b3\" : b3,\"W4\" : W4, \"b4\" : b4,}\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "     \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "     \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"leakyrelu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leakyrelu(Z)\n",
    "    \n",
    "    if activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range (1,L):\n",
    "        A_prev = A\n",
    "\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"leakyrelu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache =  linear_activation_forward(A,parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"softmax\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "     \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = - np.sum(Y*np.log(AL))/m\n",
    "    np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ , cache):\n",
    "     \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(AL, Y):\n",
    "    dZ = AL- Y\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    Z_temp = np.where(Z>0, 1, 0.01)\n",
    "    dZ = dA * Z_temp\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(Y, AL, dA, cache, activation):\n",
    "     \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"leakyrelu\":\n",
    "        dZ = leakyrelu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(AL, Y)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = -Y/AL\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(Y,AL,dAL,current_cache, \"softmax\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(Y,AL,dA_prev_temp, current_cache, \"leakyrelu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params,grads,learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = params.copy()\n",
    "\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X,Y, mini_batch_size = 64):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    inc = mini_batch_size\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "   \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k*inc:(k+1)*inc]\n",
    "        mini_batch_Y = shuffled_Y[:,k*inc:(k+1)*inc]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(Y_hat):\n",
    "    return np.argmax(Y_hat,0)\n",
    "\n",
    "def get_accuracy(predictions,Y):\n",
    "    predictions = predictions.reshape(1,predictions.shape[0])\n",
    "    #print(predictions.shape)\n",
    "    ans = 0\n",
    "    for i in range(Y.shape[1]) :\n",
    "        predict = predictions[0,i]\n",
    "        if Y[predict,i]==1 :\n",
    "            ans+=1\n",
    "    print(ans)\n",
    "    return str((ans/Y.shape[1])*100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(X, Y, learning_rate = 0.0075, num_iterations = 3000, print_cost = False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    costs -- An array of the costs of every iteration\n",
    "    Y_predict -- A one hot encoded numpy array for calculating accuracy.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    parameters = initialize_parameters(X,Y)\n",
    "    cost = 2.5\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_forward(X,parameters)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        Y_predict = np.zeros(AL.shape)\n",
    "        Y_predict[np.argmax(AL, axis = 0), np.arange(AL.shape[1])] = 1\n",
    "        \n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict - Y)) * 100))\n",
    "            print(\"accuracy : \" , get_accuracy(get_predictions(AL),Y))        \n",
    "\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters,costs, Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.302585642974819\n",
      "5156\n",
      "accuracy :  8.593333333333334%\n",
      "Cost after iteration 100: 2.3011558163568706\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 200: 2.3011516013839075\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 300: 2.301141747687725\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 400: 2.3011033446568465\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 500: 2.300350834684254\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 600: 2.070144149579087\n",
      "13211\n",
      "accuracy :  22.018333333333334%\n",
      "Cost after iteration 700: 2.251270035035066\n",
      "7157\n",
      "accuracy :  11.928333333333335%\n",
      "Cost after iteration 800: 1.2424526680424275\n",
      "28185\n",
      "accuracy :  46.975%\n",
      "Cost after iteration 900: 1.5757071058293326\n",
      "20467\n",
      "accuracy :  34.111666666666665%\n",
      "Cost after iteration 1000: 0.5046285050816177\n",
      "49035\n",
      "accuracy :  81.72500000000001%\n",
      "Cost after iteration 1100: 3.2116518083707457\n",
      "28107\n",
      "accuracy :  46.845%\n",
      "Cost after iteration 1200: 0.2576492593134629\n",
      "55773\n",
      "accuracy :  92.955%\n",
      "Cost after iteration 1300: 0.2629709061139808\n",
      "55279\n",
      "accuracy :  92.13166666666666%\n",
      "Cost after iteration 1400: 0.32535350798218726\n",
      "54214\n",
      "accuracy :  90.35666666666667%\n",
      "Cost after iteration 1500: 0.20866761675773926\n",
      "56470\n",
      "accuracy :  94.11666666666667%\n",
      "Cost after iteration 1600: 0.17021631296982215\n",
      "57100\n",
      "accuracy :  95.16666666666667%\n",
      "Cost after iteration 1700: 0.2145320534819292\n",
      "56276\n",
      "accuracy :  93.79333333333332%\n",
      "Cost after iteration 1800: 0.18372653353737403\n",
      "56804\n",
      "accuracy :  94.67333333333333%\n",
      "Cost after iteration 1900: 0.5388748750176362\n",
      "51699\n",
      "accuracy :  86.165%\n",
      "Cost after iteration 2000: 0.14676202100185817\n",
      "57457\n",
      "accuracy :  95.76166666666667%\n",
      "Cost after iteration 2100: 0.18688872148095378\n",
      "56595\n",
      "accuracy :  94.325%\n",
      "Cost after iteration 2200: 0.13332271106490712\n",
      "57692\n",
      "accuracy :  96.15333333333334%\n",
      "Cost after iteration 2300: 0.12038528361068891\n",
      "57895\n",
      "accuracy :  96.49166666666666%\n",
      "Cost after iteration 2400: 0.11745493663388569\n",
      "57939\n",
      "accuracy :  96.565%\n",
      "Cost after iteration 2499: 0.10477345682525653\n",
      "58192\n",
      "accuracy :  96.98666666666666%\n"
     ]
    }
   ],
   "source": [
    "parameters, costs, Y_predict = model(X_train,Y_train, 0.8, 2500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(parameters, X, Y):\n",
    "    AL, caches = L_forward(X,parameters)\n",
    "    print(\"accuracy : \" , get_accuracy(get_predictions(AL),Y))\n",
    "\n",
    "    return AL, Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9551\n",
      "accuracy :  95.50999999999999%\n"
     ]
    }
   ],
   "source": [
    "AL, Y_predict = accuracy_test(parameters, X_test, Y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c72393390aee9815d7b792bc598013fc10f52cf49f264bfd5dd08c1c22e3d87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
